{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers transformer_lens tqdm matplotlib pandas numpy scikit-learn seaborn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the model\n",
    "model = HookedTransformer.from_pretrained(model_path, device=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with open('contrastive_dataset.json', 'r') as f:\n",
    "    contrastive_dataset = json.load(f)\n",
    "\n",
    "with open('test_dataset.json', 'r') as f:\n",
    "    test_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"steering dataset length: \" + str(len(contrastive_dataset)))\n",
    "print(\"test dataset length: \" + str(len(test_dataset)))\n",
    "\n",
    "print(contrastive_dataset[12])\n",
    "print(test_dataset[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate with hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, layer, steering_vector=None, magnitude=0.0, max_tokens=10, temperature=0.0):\n",
    "    def steering_hook(activations, hook):\n",
    "        if steering_vector is not None:\n",
    "            prompt_length = model.to_tokens(prompt).shape[1]\n",
    "            print(f\"\\nSteering Hook Debug:\")\n",
    "            print(f\"- Activation shape: {activations.shape}\")\n",
    "            print(f\"- Prompt length: {prompt_length}\")\n",
    "            print(f\"- Steering vector shape: {steering_vector.shape}\")\n",
    "            print(f\"- Magnitude: {magnitude}\")\n",
    "\n",
    "     \n",
    "            if activations.shape[1] == 1 and not torch.isnan(steering_vector).all(): #if generated tokens forward passes 1, 1, d_model and not layer 0\n",
    "\n",
    "                print(\"Applying steering vector...\")\n",
    "                print(f\"- Pre-steering activation\")\n",
    "                print(activations[-1])\n",
    "\n",
    "                activations[0, :, :] += magnitude * steering_vector \n",
    "\n",
    "                print(f\"- Post-steering activation\") \n",
    "                print(activations[-1])                \n",
    "\n",
    "            elif torch.isnan(steering_vector).all():\n",
    "                print(\"Steering vector has all zeros - skipping steering\")\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                print(activations.shape)\n",
    "                print(\"Skipping steering (processing prompt)\")\n",
    "                \n",
    "\n",
    "        return activations\n",
    "\n",
    "    print(f\"\\nGenerate Text Debug:\")\n",
    "    print(f\"- Layer: {layer}\")\n",
    "    print(f\"- Magnitude: {magnitude}\")\n",
    "    print(f\"- Max tokens: {max_tokens}\")\n",
    "    print(f\"- Temperature: {temperature}\")\n",
    "\n",
    "    hooks = [(f\"blocks.{layer}.hook_resid_pre\", steering_hook)] if (steering_vector is not None and layer is not None) else []\n",
    "    \n",
    "    with model.hooks(hooks):\n",
    "        tokens = model.to_tokens(prompt)\n",
    "        print(f\"- Input tokens shape: {tokens.shape}\")\n",
    "        output = model.generate(\n",
    "            input=tokens,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        print(f\"- Output tokens shape: {output.shape}\")\n",
    "    \n",
    "    return model.to_string(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def parse_llama_output(output):\n",
    "    \"\"\"\n",
    "    Parse Llama output to extract the answer (A) or (B)\n",
    "    \"\"\"\n",
    "    print(f\"\\nParse Output Debug:\")\n",
    "    print(f\"Raw output type: {type(output)}\")\n",
    "    print(f\"Raw output: {output}\")\n",
    "    \n",
    "    try:\n",
    "        cleaned_output = output[0]\n",
    "        print(f\"Output: {cleaned_output}\")\n",
    "        \n",
    "        answer_part = cleaned_output.split(\"assistant<|end_header_id|>\")[-1]    \n",
    "        answer_part = answer_part.split(\"<|eot_id|>\")[0].strip()\n",
    "        \n",
    "        answer = answer_part.split()[0].strip()\n",
    "        print(f\"Final answer: {answer}\")\n",
    "        \n",
    "        if answer not in [\"(A)\", \"(B)\"]:\n",
    "            print(f\"Warning: Unexpected answer format: {answer}\")\n",
    "        \n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing output: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test steering vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def test_steering_vectors(model, test_dataset, vectors, magnitudes, output_dirs):\n",
    "    print(f\"\\nInitial vectors check:\")\n",
    "    if not vectors:\n",
    "        print(\"Warning: No steering vectors provided\")\n",
    "    for layer, vector in vectors.items():\n",
    "        print(f\"Layer {layer} vector stats:\")\n",
    "        print(f\"- Shape: {vector.shape}\")\n",
    "        print(f\"- Max: {torch.max(vector):.6f}\")\n",
    "        print(f\"- Min: {torch.min(vector):.6f}\")\n",
    "        if torch.isnan(vector).any():\n",
    "            print(f\"Warning: NaN values in vector for layer {layer}\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    layers_to_test = vectors.keys()\n",
    "    magnitudes_to_test = magnitudes\n",
    "\n",
    "    for layer in tqdm(layers_to_test, desc=\"Testing layers\", position=0, leave=True):\n",
    "        for magnitude in tqdm(magnitudes, desc=f\"Layer {layer} magnitudes\", position=1, leave=False):\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Testing Layer {layer} with magnitude {magnitude}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for item in test_dataset:\n",
    "                print(f\"\\n{'-'*30}\")\n",
    "                print(f\"Testing item {total+1}\")\n",
    "                print(f\"Question: {item['question']}\")\n",
    "                print(f\"Expected answer: {item['positive']}\")\n",
    "\n",
    "                prompt = \"\"\n",
    "                prefix = \"Reply with just (A) or (B)\"\n",
    "                prompt += f\"<|start_header_id|>system<|end_header_id|>{prefix}<|eot_id|>\"\n",
    "                prompt += f\"<|start_header_id|>user<|end_header_id|>{item['question']}\\n\\nAnswer:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "                \n",
    "                print(f\"\\nPrompt structure:\")\n",
    "                print(f\"Length: {len(prompt)}\")\n",
    "                print(f\"Tokens: {model.to_tokens(prompt).shape}\")\n",
    "\n",
    "                steering_vector = vectors[layer]\n",
    "                \n",
    "                output = generate_text(\n",
    "                    model, \n",
    "                    prompt=prompt, \n",
    "                    layer=layer, \n",
    "                    steering_vector=steering_vector, \n",
    "                    magnitude=magnitude,\n",
    "                    max_tokens=20,\n",
    "                    temperature=0.0\n",
    "                )\n",
    "                print(f\"\\nRaw output: {output}\")\n",
    "                \n",
    "                answer = parse_llama_output(output)\n",
    "                \n",
    "                if answer:\n",
    "                    correct += int(answer == item[\"positive\"])\n",
    "                    print(f\"\\nResult: {'✓ Correct' if answer == item['positive'] else '✗ Incorrect'}\")\n",
    "                    print(f\"\\nExpected: {item['positive']}, Got: {answer}\")\n",
    "                else:\n",
    "                    print(f\"\\nWarning: Couldn't parse output: {output}\")\n",
    "                total += 1\n",
    "            \n",
    "            accuracy = correct/total\n",
    "            print(f\"\\nSummary for Layer {layer}, Magnitude {magnitude}:\")\n",
    "            print(f\"Correct: {correct}/{total}\")\n",
    "            print(f\"Accuracy: {accuracy:.2%}\")\n",
    "            \n",
    "            results.append({\n",
    "                'layer': layer,\n",
    "                'magnitude': magnitude,\n",
    "                'accuracy': accuracy\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nFinal Results DataFrame:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    filename = 'steering_results.csv'\n",
    "    results_df.to_csv(\n",
    "        os.path.join(output_dirs, filename), \n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_baseline_vectors(model, test_dataset, output_dirs):\n",
    "    results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for item in test_dataset:\n",
    "        print(f\"\\nTesting item {total+1}\")\n",
    "        print(f\"Question: {item['question']}\")\n",
    "        print(f\"Expected answer: {item['positive']}\")\n",
    "\n",
    "        prompt = \"\"\n",
    "        prefix = \"Reply with just (A) or (B)\"\n",
    "        prompt += f\"<|start_header_id|>system<|end_header_id|>{prefix}<|eot_id|>\"\n",
    "        prompt += f\"<|start_header_id|>user<|end_header_id|>{item['question']}\\n\\nAnswer:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "        \n",
    "        output = generate_text(\n",
    "            model, \n",
    "            prompt=prompt, \n",
    "            layer=None, \n",
    "            steering_vector=None, \n",
    "            magnitude=0.0,\n",
    "            max_tokens=20,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        \n",
    "        answer = parse_llama_output(output)\n",
    "        \n",
    "        if answer:\n",
    "            correct += int(answer == item[\"positive\"])\n",
    "        total += 1\n",
    "    \n",
    "    accuracy = correct/total\n",
    "    results.append({\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "    print(accuracy)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(os.path.join(output_dirs, 'baseline_results.csv'), index=False)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_vectors_from_dir(directory):\n",
    "    \"\"\"Load all vectors from a directory\"\"\"\n",
    "    vectors = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith('layer_') and filename.endswith('.pt'):\n",
    "            layer = int(filename.split('_')[1].split('.')[0])\n",
    "            vectors[layer] = torch.load(os.path.join(directory, filename))\n",
    "            print(vectors[layer])\n",
    "    return vectors\n",
    "\n",
    "existing_dir = './0250120_093303'\n",
    "if existing_dir is not None:\n",
    "    normalized_vectors = load_vectors_from_dir(os.path.join(existing_dir, 'normalized_vectors'))\n",
    "    print(f\"Loaded {len(normalized_vectors)} normalized vectors from {existing_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nTesting model baseline\")\n",
    "baseline_results = test_baseline_vectors(\n",
    "    model, \n",
    "    test_dataset,\n",
    "    output_dirs=existing_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## steering test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nTesting steering vectors...\")\n",
    "magnitudes = np.arange(-3, 3.5, 0.5)\n",
    "steering_results = test_steering_vectors(\n",
    "    model, \n",
    "    test_dataset, \n",
    "    vectors=normalized_vectors, \n",
    "    magnitudes=magnitudes, \n",
    "    output_dirs=existing_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "for layer in steering_results['layer'].unique():\n",
    "    layer_data = steering_results[steering_results['layer'] == layer]\n",
    "    plt.plot(layer_data['magnitude'], layer_data['accuracy'], label=f'Layer {layer}')\n",
    "\n",
    "plt.xlabel('Steering Magnitude')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Steering Effect by Layer and Magnitude')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dirs['results'], 'steering_plot.png'))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate subplot grid dimensions\n",
    "n_layers = len(steering_results['layer'].unique())\n",
    "n_cols = 6  # You can adjust this\n",
    "n_rows = (n_layers + n_cols - 1) // n_cols  # Ceiling division\n",
    "\n",
    "# Create subplot figure\n",
    "plt.figure(figsize=(15, 4*n_rows))\n",
    "\n",
    "for i, layer in enumerate(sorted(steering_results['layer'].unique())):\n",
    "    # Create subplot\n",
    "    plt.subplot(n_rows, n_cols, i+1)\n",
    "    \n",
    "    # Plot data for this layer\n",
    "    layer_data = steering_results[steering_results['layer'] == layer]\n",
    "    plt.plot(layer_data['magnitude'], layer_data['accuracy'], 'b-o')\n",
    "    \n",
    "    # Add horizontal line for baseline accuracy\n",
    "    plt.axhline(y=baseline_accuracy, color='r', linestyle='--', label='Baseline')\n",
    "    \n",
    "    # Customize subplot\n",
    "    plt.title(f'Layer {layer}')\n",
    "    plt.xlabel('Steering Magnitude')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dirs['results'], 'steering_plot_subplots.png'))\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
