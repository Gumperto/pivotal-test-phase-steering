{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers transformer_lens tqdm matplotlib pandas numpy scikit-learn seaborn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the model\n",
    "model = HookedTransformer.from_pretrained(model_path, device=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with open('contrastive_dataset.json', 'r') as f:\n",
    "    contrastive_dataset = json.load(f)\n",
    "\n",
    "with open('test_dataset.json', 'r') as f:\n",
    "    test_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"steering dataset length: \" + str(len(contrastive_dataset)))\n",
    "print(\"test dataset length: \" + str(len(test_dataset)))\n",
    "\n",
    "print(contrastive_dataset[12])\n",
    "print(test_dataset[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_activation(model, tokens, layer, token_type):\n",
    "    \"\"\"\n",
    "    Extract residual stream activation at the layer and last A or B token position.\n",
    "    token_type: either \"A\" or \"B\"\n",
    "    \"\"\"\n",
    "    stored_activation = None\n",
    "\n",
    "    def hook_fn(activation, hook):\n",
    "        nonlocal stored_activation\n",
    "        \n",
    "        # Find all positions of A or B\n",
    "        token_positions = (tokens == tokenizer.convert_tokens_to_ids(token_type)).nonzero()[-1]        \n",
    "        # print(tokens)\n",
    "        # print(tokens == tokenizer.convert_tokens_to_ids(token_type))\n",
    "        \n",
    "        if len(token_positions) == 0:\n",
    "            print(\"WARNING: No positions found for token type!\")\n",
    "            return activation\n",
    "        # print(token_positions)\n",
    "            \n",
    "        # Take the last position\n",
    "        last_pos = token_positions[-1]\n",
    "\n",
    "        print(f\"Extracting activation for token '{token_type}' at position {last_pos}\")\n",
    "        \n",
    "        \n",
    "        stored_activation = activation[:, last_pos, :].clone().detach()\n",
    "        return activation\n",
    "\n",
    "    model.reset_hooks()\n",
    "\n",
    "    with model.hooks(fwd_hooks=[(f\"blocks.{layer}.hook_resid_pre\", hook_fn)]):\n",
    "        with torch.no_grad():\n",
    "            model(tokens)\n",
    "    return stored_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#test\n",
    "pos_answer = '(A)'\n",
    "prefix = \"Reply with just (A) or (B)\"\n",
    "prompt = f\"<|start_header_id|>system<|end_header_id|>{prefix}<|eot_id|><|start_header_id|>user<|end_header_id|>Question 1: Are you being  tested? Hello Hdfsfsdello\\n\\nAnswer:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "pos_prompt = f\"{prompt}\\n\\n (A)\"\n",
    "\n",
    "pos_tokens = model.to_tokens(pos_prompt)\n",
    "\n",
    "pos_token_type = pos_answer[:2]\n",
    "pos_activation = extract_activation(model, pos_tokens, 10, pos_token_type)\n",
    "print(pos_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_steering_vector(model, dataset, layer):\n",
    "    \"\"\"\n",
    "    Generate steering vector from contrastive dataset.\n",
    "    \"\"\"\n",
    "    pos_activations = []\n",
    "    neg_activations = []\n",
    "\n",
    "    for item in dataset:\n",
    "  \n",
    "        question = item[\"question\"]\n",
    "        print(question)\n",
    "        pos_answer = item[\"positive\"]\n",
    "        neg_answer = item[\"negative\"]\n",
    "\n",
    "\n",
    "        print(\"\\n--- Processing Example ---\")\n",
    "        # print(f\"Question: {item['question']}\")\n",
    "        # print(f\"Positive answer: {pos_answer}\")\n",
    "        # print(f\"Negative answer: {neg_answer}\")\n",
    "        # print(f\"Layer: {layer}\")\n",
    "            \n",
    "        prefix = \"Reply with just (A) or (B)\"\n",
    "        prompt = f\"<|start_header_id|>system<|end_header_id|>{prefix}<|eot_id|><|start_header_id|>user<|end_header_id|>{question}\\n\\nAnswer:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "\n",
    "        pos_prompt = f\"{prompt}\\n\\n {pos_answer}\"  # needs space or the tokenisation messes up\n",
    "        neg_prompt = f\"{prompt}\\n\\n {neg_answer}\"  \n",
    "        \n",
    "        \n",
    "        #pos_prompt = f\"{question}\\nAnswer: {pos_answer}\" old\n",
    "        #neg_prompt = f\"{question}\\nAnswer: {neg_answer}\"\n",
    "        \n",
    "\n",
    "        pos_tokens = model.to_tokens(pos_prompt)\n",
    "        neg_tokens = model.to_tokens(neg_prompt)\n",
    "\n",
    "        #print(\"\\nDebug prompts:\")\n",
    "        #print(f\"Positive prompt: {pos_prompt}\")\n",
    "        #print(f\"Negative prompt: {neg_prompt}\")\n",
    "\n",
    "        pos_token_type = pos_answer[1]\n",
    "        neg_token_type = neg_answer[1]\n",
    "\n",
    "        \n",
    "        model.reset_hooks()\n",
    "\n",
    "        pos_activation = extract_activation(model, pos_tokens, layer, pos_token_type)\n",
    "        \n",
    "\n",
    "        model.reset_hooks()\n",
    "\n",
    "        neg_activation = extract_activation(model, neg_tokens, layer, neg_token_type)        \n",
    "        \n",
    "        print(pos_activation)\n",
    "        print(neg_activation)\n",
    "        \n",
    "        pos_activations.append(pos_activation)\n",
    "        neg_activations.append(neg_activation)\n",
    "\n",
    "    pos_mean = torch.stack(pos_activations).mean(dim=0)\n",
    "    neg_mean = torch.stack(neg_activations).mean(dim=0)\n",
    "\n",
    "    print(f\"\\n-----Steering done for {layer}!!-----\\n\")\n",
    "\n",
    "    return pos_mean - neg_mean\n",
    "\n",
    "def normalize_vector(vector):\n",
    "    \"\"\"\n",
    "    Normalize a single vector to unit norm\n",
    "    \"\"\"\n",
    "    return vector * (1.0 / vector.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate and save vectors for all layers\n",
    "def generate_all_layers(model, dataset, layers, output_dirs):\n",
    "    raw_vectors = {}\n",
    "    normalized_vectors = {}\n",
    "    # Use position parameter in tqdm to control where the progress bar appears\n",
    "    for layer in tqdm(layers, desc=\"Processing layers\", position=0, leave=True):\n",
    "        vector = generate_steering_vector(model, dataset, layer)\n",
    "        raw_vectors[layer] = vector\n",
    "        normalized_vectors[layer] = normalize_vector(vector)\n",
    "        \n",
    "        # Save both versions\n",
    "        torch.save(\n",
    "            vector, \n",
    "            os.path.join(output_dirs['vectors'], f\"layer_{layer}.pt\")\n",
    "        )\n",
    "        \n",
    "        torch.save(\n",
    "            normalized_vectors[layer], \n",
    "            os.path.join(output_dirs['normalized_vectors'], f\"layer_{layer}.pt\")\n",
    "        )\n",
    "    \n",
    "    return raw_vectors, normalized_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def setup_output_dirs(existing):\n",
    "    \"\"\"Create output directories with timestamp\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    if existing is not None:\n",
    "        base_dir = f\"outputs_{timestamp}\"\n",
    "        return base_dir\n",
    "    base_dir = timestamp\n",
    "    \n",
    "    # Create directories\n",
    "    dirs = {\n",
    "        'base': base_dir,\n",
    "        'vectors': os.path.join(base_dir, 'vectors'),\n",
    "        'normalized_vectors': os.path.join(base_dir, 'normalized_vectors'),\n",
    "        'results': os.path.join(base_dir, 'results')\n",
    "    }\n",
    "    \n",
    "    for dir_path in dirs.values():\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "    return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def run_experiment(model, contrastive_dataset):\n",
    "    output_dirs = setup_output_dirs(existing=None)\n",
    "   \n",
    "    with open(os.path.join(output_dirs['base'], 'experiment_info.txt'), 'w') as f:\n",
    "        f.write(f\"Model: {model.cfg.model_name}\\n\")\n",
    "        f.write(f\"Number of layers: {model.cfg.n_layers}\\n\")\n",
    "        f.write(f\"Training dataset size: {len(contrastive_dataset)}\\n\")\n",
    "        f.write(f\"Test dataset size: {len(test_dataset)}\\n\")\n",
    "\n",
    "    layers = range(model.cfg.n_layers)\n",
    "    raw_vectors, normalized_vectors = generate_all_layers(model, contrastive_dataset, layers, output_dirs)\n",
    "\n",
    "    print(\"Extracted all vectors\")\n",
    "    \n",
    "    return output_dirs, raw_vectors, normalized_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "output_dirs, raw_vectors, normalised_vectors = run_experiment(model, contrastive_dataset)\n",
    "print(f\"Experiment results saved to: {output_dirs['base']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
